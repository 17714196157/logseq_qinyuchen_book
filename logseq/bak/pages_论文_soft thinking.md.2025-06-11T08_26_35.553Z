- Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space
  论文地址：  https://arxiv.org/abs/2505.15778
  项目地址：  https://github.com/eric-ai-lab/Soft-Thinking
-
- ### 背景：为什么要摆脱“离散 token”限制？
  大语言模型（LLM）在复杂推理任务中大放异彩，而链式思考（Chain-of-Thought, CoT）通过生成中间步骤进一步提升了推理表现。 但标准 CoT 依然存在四大痛点：
	- 1. **离散符号受限**
	- 每步仅选出一个固定 token，表达粒度受限于人为定义的词表。
	  
	  2. **背离人类认知**
	- 神经科学研究表明，人脑推理依赖抽象概念而非单个词元。
	  
	  3. **早期决策 + 单路径**
	- 序列式采样迫使模型过早锁定一路径，一旦偏离就难以回溯。
	  
	  4. **高不确定任务易失误**
	- 多条可行路线时，模型可能走错分支，既浪费 token 也降低准确率。
	-
- � Soft Thinking: 走向连续概念空间
  
  为打破离散局限，论文提出 Soft Thinking —— 一个无需额外训练的推理范式。
- 为了解决LLMs在离散空间推理的限制，论文提出了 Soft Thinking 方法。这是一个无需额外训练（training-free）的方法，旨在让LLMs能够在**连续的概念空间**（continuous concept space）中进行推理。
- Soft Thinking的核心创新点在于，它取代了标准CoT中离散的词元选择过程。在标准的CoT中，模型在每一步推理后会生成一个对词汇表所有词元的概率分布，然后从中“硬”地选择（采样）一个词元作为下一步的输入。Soft Thinking则不同，它保留了**原始完整的概率分布**。
- 传统CoT
	- 每步仅选择一个最高概率token
	- 单一推理路径
- 连续CoT vs Soft Thinking
	- 保留整个词汇表的概率分布
	- 连续概念空间推理
	- 同时涵盖多个潜在路径
- ### PipeLine:
  模型正常处理输入提示，直到达到指定的思考阶段。
  在每个中间思考步骤中，模型不是选择单个 token，而是生成一个关于词汇表的概率分布（即“概念 token”）。
  此概念 token 用于计算加权嵌入，方法是根据其概率对所有 token 嵌入进行加权求和。
  加权嵌入作为输入被注入回模型，用于下一步推理。
  此过程持续多个步骤，模型在连续概念空间中运行。
  对于最终答案生成，模型恢复为标准离散 token 选择。
  该方法是免训练的，这意味着它可以应用于现有的预训练模型，而无需任何微调或额外训练。
- ### 软思考和传统 CoT 之间的主要区别：
  中间步骤的表示：CoT 使用离散 token，而软思考使用概率分布。
  推理路径的探索：CoT 遵循单一路径，而软思考隐式地探索多个潜在路径。
  Token 效率：与 CoT 相比，软思考通常需要更少的 token 即可达到解决方案。
  不确定性的处理：软思考更好地表示了推理过程中的不确定性。
  连续概念空间是另外一个核心观念：软思考方法的基本要素。该空间被定义为模型词汇表中所有 token 嵌入的凸组合。从数学上讲，对于具有嵌入 {e₁, e₂, ..., eᵥ} 的大小为 V 的词汇表，
  连续概念空间中的任何点都可以表示为：
  e = ∑ᵢ pᵢeᵢ
  其中 pᵢ 表示分配给 token i 的概率，并且 ∑ᵢ pᵢ = 1。
  这种连续空间允许表示可能无法被词汇表中的任何单个 token 完美捕获的抽象概念。通过在此空间中运行，模型可以在适当的情况下保持歧义，并表达位于离散 token “之间”的概念。
- ##### 软思考 VS 传统 CoT 总结：
  软思考减少了 LLM 里面的反思重新再来行为，提升了 token 效率，
- 传统：选择一条路径 → 发现错误 → 需要重新开始 → 浪费tokens，
  而Soft Thinking：同时保持多个选项的概率权重 → 自然收敛到正确路径。