title:: 论文:Soft Thinking

- Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space
  论文地址：  https://arxiv.org/abs/2505.15778
  项目地址：  https://github.com/eric-ai-lab/Soft-Thinking
- PipeLine:
  模型正常处理输入提示，直到达到指定的思考阶段。
  在每个中间思考步骤中，模型不是选择单个 token，而是生成一个关于词汇表的概率分布（即“概念 token”）。
  此概念 token 用于计算加权嵌入，方法是根据其概率对所有 token 嵌入进行加权求和。
  加权嵌入作为输入被注入回模型，用于下一步推理。
  此过程持续多个步骤，模型在连续概念空间中运行。
  对于最终答案生成，模型恢复为标准离散 token 选择。
  该方法是免训练的，这意味着它可以应用于现有的预训练模型，而无需任何微调或额外训练。
- 软思考和传统 CoT 之间的主要区别包括：
  中间步骤的表示：CoT 使用离散 token，而软思考使用概率分布。
  推理路径的探索：CoT 遵循单一路径，而软思考隐式地探索多个潜在路径。
  Token 效率：与 CoT 相比，软思考通常需要更少的 token 即可达到解决方案。
  不确定性的处理：软思考更好地表示了推理过程中的不确定性。
  连续概念空间是另外一个核心观念：软思考方法的基本要素。该空间被定义为模型词汇表中所有 token 嵌入的凸组合。从数学上讲，对于具有嵌入 {e₁, e₂, ..., eᵥ} 的大小为 V 的词汇表，
  连续概念空间中的任何点都可以表示为：
  e = ∑ᵢ pᵢeᵢ
  其中 pᵢ 表示分配给 token i 的概率，并且 ∑ᵢ pᵢ = 1。
  这种连续空间允许表示可能无法被词汇表中的任何单个 token 完美捕获的抽象概念。通过在此空间中运行，模型可以在适当的情况下保持歧义，并表达位于离散 token “之间”的概念。
- 软思考 VS 传统 CoT 总结：
  软思考减少了 LLM 里面的反思重新再来行为，提升了 token 效率，传统：选择一条路径 → 发现错误 → 需要重新开始 → 浪费tokens，
  而Soft Thinking：同时保持多个选项的概率权重 → 自然收敛到正确路径。