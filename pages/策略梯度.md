- 强化学习中的策略梯度（Policy Gradient）是一种非常重要的方法，用于优化智能体的策略，以最大化期望累积回报。
- ### 1. 基本概念
  在强化学习中，智能体（Agent）需要在环境中（Environment）通过一系列动作（Actions）来获得奖励（Rewards）。策略（Policy）是指智能体选择动作的概率分布，通常用 \(\pi(a|s)\) 表示，即在状态 \(s\) 下选择动作 \(a\) 的概率。
  
  策略梯度方法的目标是**直接优化这个策略函数，使得智能体能够更好地选择动作，从而获得更高的累积奖励。**
### 2. 策略梯度的核心思想
策略梯度的核心思想是通过梯度上升来优化策略函数。具体来说，我们希望最大化期望累积奖励 \(J(\theta)\)，其中 \(\theta\) 是策略函数的参数。策略梯度方法通过计算 \(J(\theta)\) 关于 \(\theta\) 的梯度 \(\nabla_\theta J(\theta)\)，然后沿着梯度方向更新参数，从而优化策略。

期望累积奖励 \(J(\theta)\) 可以表示为：
\[ J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^{T} \gamma^t R_t \right] \]
其中：
- \(\pi_\theta\) 是参数为 \(\theta\) 的策略。
- \(\gamma\) 是折扣因子，用于衡量未来奖励的重要性。
- \(R_t\) 是在时间步 \(t\) 获得的奖励。
### 3. 策略梯度的计算
策略梯度的计算基于似然比技巧（Likelihood Ratio Trick）。策略梯度定理（Policy Gradient Theorem）给出了策略梯度的解析形式：
\[ \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot Q^{\pi_\theta}(s_t, a_t) \right] \]
其中：
- \(\nabla_\theta \log \pi_\theta(a_t|s_t)\) 是策略函数的对数概率关于参数 \(\theta\) 的梯度，表示在状态 \(s_t\) 下选择动作 \(a_t\) 的概率的对数对参数的敏感度。
- \(Q^{\pi_\theta}(s_t, a_t)\) 是在策略 \(\pi_\theta\) 下，从状态 \(s_t\) 开始选择动作 \(a_t\) 的期望累积奖励，即动作价值函数。
-
- ##### 在强化学习中，当选择用策略梯度最大化期望奖励时，应该使用什么方法？
  梯度上升法，因为要让期望奖励越大越好，所以是**梯度上升法**。
  梯度上升法在更新参数的时候要添加梯度信息。
  直接通过计算策略的期望奖励梯度，沿梯度方向更新参数。
  核心公式为：
  \[ \theta^* = \theta + \alpha \nabla_\theta J(\theta) \]
  其中，\(\nabla_\theta J(\theta)\) 通过蒙特卡洛采样估计，适用于连续动作空间和随机策略。
-