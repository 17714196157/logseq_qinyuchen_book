collapsed:: true
1. 强化学习技术概览 
2. 马尔可夫决策过程
3. 值函数估计 
4. 无模型控制方法 
5. 参数化值函数 
6. 策略梯度 
7. 深度强化学习 – 价值方法 
8. 深度强化学习 – 策略方法

-
- ### 强化学习的定义：
- 学习做什么(即如何把当前的情境映射成动作)才能使得数值化的收益信号最大。---出自《Reinforcement Learning: An Introduction》
- 强化学习的两个实体：智能体（Agent）与环境（Environment）
- ![image.png](../assets/image_1744702869766_0.png)
- **状态空间S**：S即为State，指环境中所有可能状态的集合
- **动作选择空间A**：A即为Action space，指智能体所有可能动作的集合
- **奖励R**：R即为Reward，指智能体在环境的某一状态下所获得的奖励。
- **策略P**：P即为Policy， 是智能体的行为方式
- 智能体在这个过程中学习，它的最终目标是：
  **找到一个策略，这个策略根据当前观测到的环境状态和奖励反馈，来选择最佳的动作。**
-
- ##### 强化学习与传统model有监督学习的区别：
- [[强化学习VS有监督学习]]
-
- ##### 如何描述序列决策型任务
- [[MDP马尔科夫决策过程]]
-
- ##### 价值函数
- [[价值函数]]
-
- ##### 如何优化策略
- 策略优化的理论基础
- [[策略提升定理]]
-
- ##### 强化学习的底层逻辑
  人在学新东西的时候是怎么学的?
  1. 跟老师学   --监督学习
  2.借鉴以往类似经验、知识尝试
  3.探索、总结   ---强化学习
  **策略学习思路:根据探索得到的回报不断优化策略**
-
- 举例说明：
- ![image.png](../assets/image_1744821615671_0.png){:height 250, :width 450}
- 策略学习的底层逻辑:根据回报更新策略，复盘式学习，训练过程依赖回报
- ![微信截图_20250417004122.png](../assets/微信截图_20250417004122_1744821710810_0.png){:height 200, :width 400}
-
- #### 从知道什么是好的，到如何做好行动?
- 从知道什么是好的：估计 \( V^\pi(S_t) \)
  \[
  V(S_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t))
  \]
  \[
  V(S_t) \leftarrow V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t))
  \]
	- 1)基于V函数，如何选择好的行动？
	  \[
	  \pi(s) = \arg \max_{a \in A} \sum_{s' \in S} P_{sa}(s') V(s')
	  \]
	                     需要知道环境模型，状态转移矩阵
	- 2)基于Q函数，如何选择好的行动？
	  \[
	  \pi(s) = \arg \max_{a \in A} Q(s, a)
	  \]
-
-
-
- 强化学习训练方法的分类
- [[强化学习算法的分类]]
-
- 强化学习算法举例说明：
- 1) 针对状态函数、动作函数是离散的，优化方法：
- [[Q-learning算法]]
-
- 2) 针对状态函数、动作函数是连续的，优化方法：
- [[策略梯度]]
- [[Actor-Critic 算法]]